{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Density Network\n",
    "\n",
    "Reproducing the classic Bishop MDN network tasks in Keras. The idea in this task is to predict a the value of an inverse sine function. This function has multiple real-valued solutions at each point, so the ANN model needs to have the capacity to handle this in it's loss function. An MDN is a good way to handle the predictions of these multiple output values.\n",
    "\n",
    "There's a couple of other versions of this task, and this implementation owes much to the following:\n",
    "\n",
    "- [David Ha - Mixture Density Networks with TensorFlow](http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/)\n",
    "- [Mixture Density Networks in Edward](http://edwardlib.org/tutorials/mixture-density-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal imports for everybody\n",
    "from context import * # imports the MDN layer \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating some data:\n",
    "NSAMPLE = 3000\n",
    "\n",
    "y_data = np.float32(np.random.uniform(-10.5, 10.5, NSAMPLE))\n",
    "r_data = np.random.normal(size=NSAMPLE)\n",
    "x_data = np.sin(0.75 * y_data) * 7.0 + y_data * 0.5 + r_data * 1.0\n",
    "x_data = x_data.reshape((NSAMPLE, 1))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_data,y_data,'ro', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MDN Model\n",
    "\n",
    "Now we will construct the MDN model in Keras. This uses the `Sequential` model interface in Keras.\n",
    "\n",
    "The `MDN` layer comes after one or more `Dense` layers. You need to define the output dimension and number of mixtures for the MDN like so: `MDN(output_dimension, number_mixtures)`.\n",
    "\n",
    "For this problem, we only need an output dimension of 1 as we are predicting one value (y). Adding more mixtures adds a more parameters (model is more complex, takes longer to train), but might help make the solutions better. You can see from the training data that there are at maximum 5 different layers to predict in the curve, so setting `N_MIXES = 5` is a good place to start.\n",
    "\n",
    "For MDNs, we have to use a special loss function that can handle the mixture parameters: the function has to take into account the number of output dimensions and mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN = 15\n",
    "N_MIXES = 10\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN, batch_input_shape=(None, 1), activation='relu'))\n",
    "model.add(keras.layers.Dense(N_HIDDEN, activation='relu'))\n",
    "model.add(mdn.MDN(1, N_MIXES))\n",
    "model.compile(loss=mdn.get_mixture_loss_func(1,N_MIXES), optimizer=keras.optimizers.Adam()) #, metrics=[mdn.get_mixture_mse_accuracy(1,N_MIXES)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Now we train the model using Keras' normal `fit` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=x_data, y=y_data, batch_size=128, epochs=500, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if you want to.\n",
    "# model.save(\"MDN-1D-sine-prediction-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model if you want to.\n",
    "# To load models from file, you need to supply the layer and loss function as custom_objects:\n",
    "# model = keras.models.load_model('MDN-1D-sine-prediction-model.h5', custom_objects={'MDN': mdn.MDN, 'loss_func': mdn.get_mixture_loss_func(1, N_MIXES)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loss\n",
    "\n",
    "It's interesting to see how the model trained. We can see that after a certain point training is rather slow.\n",
    "\n",
    "For this problem a loss value around 1.5 produces quite good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylim([0,9])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Functions\n",
    "\n",
    "The MDN model outputs parameters of a mixture model---a list of means (mu), variances (sigma), and weights (pi).\n",
    "\n",
    "The `mdn` module provides a function to sample from these parameters as follows. First the parameters are split up into `mu`s, `sigma`s and `pi`s, then the categorical distribution formed by the `pi`s is sampled to choose which mixture component should be sampled, then that component's `mu`s and `sigma`s is used to sample from a multivariate normal model, here's the code:\n",
    "\n",
    "    def sample_from_output(params, output_dim, num_mixes, temp=1.0):\n",
    "        \"\"\"Sample from an MDN output with temperature adjustment.\"\"\"\n",
    "        mus = params[:num_mixes*output_dim]\n",
    "        sigs = params[num_mixes*output_dim:2*num_mixes*output_dim]\n",
    "        pis = softmax(params[-num_mixes:], t=temp)\n",
    "        m = sample_from_categorical(pis)\n",
    "        # Alternative way to sample from categorical:\n",
    "        # m = np.random.choice(range(len(pis)), p=pis)\n",
    "        mus_vector = mus[m*output_dim:(m+1)*output_dim]\n",
    "        sig_vector = sigs[m*output_dim:(m+1)*output_dim] * temp  # adjust for temperature\n",
    "        cov_matrix = np.identity(output_dim) * sig_vector\n",
    "        sample = np.random.multivariate_normal(mus_vector, cov_matrix, 1)\n",
    "        return sample\n",
    "        \n",
    "If you only have one prediction to sample from, you can use the function as is; but if you need to sample from a lot of predictions at once (as in the following sections), you can use `np.apply_along_axis` to apply it to a whole numpy array of predicted parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the MDN Model\n",
    "\n",
    "Now we try out the model by making predictions at 3000 evenly spaced points on the x-axis. \n",
    "\n",
    "Mixture models output lists of parameters, so we're going to sample from these parameters for each point on the x-axis, and also try plotting the parameters themselves so we can have some insight into what the model is learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample on some test data:\n",
    "x_test = np.float32(np.arange(-15,15,0.01))\n",
    "NTEST = x_test.size\n",
    "print(\"Testing:\", NTEST, \"samples.\")\n",
    "x_test = x_test.reshape(NTEST,1) # needs to be a matrix, not a vector\n",
    "\n",
    "# Make predictions from the model\n",
    "y_test = model.predict(x_test)\n",
    "# y_test contains parameters for distributions, not actual points on the graph.\n",
    "# To find points on the graph, we need to sample from each distribution.\n",
    "\n",
    "# Sample from the predicted distributions\n",
    "y_samples = np.apply_along_axis(mdn.sample_from_output, 1, y_test, 1, N_MIXES,temp=1.0)\n",
    "\n",
    "# Split up the mixture parameters (for future fun)\n",
    "mus = np.apply_along_axis((lambda a: a[:N_MIXES]),1, y_test)\n",
    "sigs = np.apply_along_axis((lambda a: a[N_MIXES:2*N_MIXES]),1, y_test)\n",
    "pis = np.apply_along_axis((lambda a: mdn.softmax(a[2*N_MIXES:])),1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the samples\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_data,y_data,'ro', x_test, y_samples[:,:,0], 'bo',alpha=0.3)\n",
    "plt.show()\n",
    "# These look pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the means - this gives us some insight into how the model learns to produce the mixtures.\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_data,y_data,'ro', x_test, mus,'bo',alpha=0.3)\n",
    "plt.show()\n",
    "# Cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the variances and weightings of the means as well.\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "# ax1.scatter(data[0], data[1], marker='o', c='b', s=data[2], label='the data')\n",
    "ax1.scatter(x_data,y_data,marker='o', c='r', alpha=0.3)\n",
    "for i in range(N_MIXES):\n",
    "    ax1.scatter(x_test, mus[:,i], marker='o', s=200*sigs[:,i]*pis[:,i],alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
